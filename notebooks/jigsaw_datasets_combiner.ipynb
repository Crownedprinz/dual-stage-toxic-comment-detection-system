{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f590d1a",
   "metadata": {},
   "source": [
    "Download the jigsaw unintended file if it doesn't exist yet. It only downloads once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a82e9224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/crownedprinz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../raw_data/jigsaw_unintended/train.csv already exists; skipping download.\n",
      "../processed_data/jigsaw_unintended/cleaned_train.csv already exists; skipping processing.\n",
      "test_private_expanded.csv already exists; skipping download.\n",
      "test_public_expanded.csv already exists; skipping download.\n",
      "test.csv already exists; skipping download.\n",
      "All files are downloaded and ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gdown\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Download NLTK resources if not already available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy model (make sure to run: python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n",
    "\n",
    "def vectorized_clean(series):\n",
    "    \"\"\"\n",
    "    Use vectorized operations to lowercase and remove punctuation.\n",
    "    \"\"\"\n",
    "    pattern = f\"[{re.escape(string.punctuation)}]\"\n",
    "    return series.str.lower().str.replace(pattern, '', regex=True)\n",
    "\n",
    "def spacy_tokenize(texts, batch_size=500):\n",
    "    \"\"\"\n",
    "    Use spaCy's nlp.pipe to tokenize text in batches.\n",
    "    \"\"\"\n",
    "    clean_texts = []\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=batch_size, n_process=1), total=len(texts)):\n",
    "        tokens = [token.text for token in doc if not token.is_space]\n",
    "        clean_texts.append(\" \".join(tokens))\n",
    "    return clean_texts\n",
    "\n",
    "# Create directories if they don't exist\n",
    "raw_dir = os.path.join(\"../raw_data\", \"jigsaw_unintended\")\n",
    "processed_dir = os.path.join(\"../processed_data\", \"jigsaw_unintended\")\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Download train.csv using the direct download link\n",
    "train_url = 'https://drive.google.com/uc?export=download&id=1N-orSYsJCubW2SXLXVukT9zfFf8aAg-C'\n",
    "train_file = os.path.join(raw_dir, 'train.csv')\n",
    "cleaned_train = os.path.join(processed_dir, 'cleaned_train.csv')\n",
    "\n",
    "# Check if the file exists before downloading\n",
    "if not os.path.exists(train_file):\n",
    "    gdown.download(train_url, train_file, quiet=False)\n",
    "else:\n",
    "    print(f\"{train_file} already exists; skipping download.\")\n",
    "\n",
    "\n",
    "# Check if the cleaned file already exists\n",
    "if os.path.exists(cleaned_train):\n",
    "    print(f\"{cleaned_train} already exists; skipping processing.\")\n",
    "else:\n",
    "    # Process the file in chunks if it's too large\n",
    "    chunk_size = 10**6  # Adjust based on your memory capacity\n",
    "    processed_chunks = []\n",
    "\n",
    "    for chunk in pd.read_csv(train_file, chunksize=chunk_size):\n",
    "        # Remove duplicates and fill missing values\n",
    "        chunk.drop_duplicates(inplace=True)\n",
    "        chunk.fillna('', inplace=True)\n",
    "        \n",
    "        if 'comment_text' in chunk.columns:\n",
    "            # Vectorized cleaning: lowercase & remove punctuation\n",
    "            chunk['comment_text'] = vectorized_clean(chunk['comment_text'])\n",
    "            \n",
    "            # Option 1: If tokenization isn't strictly needed here, comment out the next block.\n",
    "            # Option 2: If tokenization is needed, use spaCy's nlp.pipe for improved performance.\n",
    "            texts = chunk['comment_text'].tolist()\n",
    "            chunk['comment_text'] = spacy_tokenize(texts, batch_size=500)\n",
    "        \n",
    "        processed_chunks.append(chunk)\n",
    "\n",
    "    # Concatenate all processed chunks and save the cleaned data\n",
    "    df = pd.concat(processed_chunks)\n",
    "    cleaned_train = os.path.join(processed_dir, 'cleaned_train.csv')\n",
    "    df.to_csv(cleaned_train, index=False)\n",
    "\n",
    "    print(f\"Cleaned data saved to {cleaned_train}\")\n",
    "\n",
    "# Define file names and their corresponding Google Drive direct download URLs\n",
    "files = {\n",
    "    \"test_private_expanded.csv\": \"https://drive.google.com/uc?export=download&id=1bAOCveaQZ1s0WI3OKBDh_xevKqtLB5vv\",\n",
    "    \"test_public_expanded.csv\": \"https://drive.google.com/uc?export=download&id=1w4Sh6m16BttINP3aVqbFtpQcvECtilho\",\n",
    "    \"test.csv\": \"https://drive.google.com/uc?export=download&id=1oivL4ZYsABBqbDFM3KyZJH8mOHPx_q7O\"\n",
    "}\n",
    "\n",
    "# Download each file if it does not exist locally\n",
    "for filename, url in files.items():\n",
    "    filepath = os.path.join(raw_dir, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        gdown.download(url, filepath, quiet=False)\n",
    "    else:\n",
    "        print(f\"{filename} already exists; skipping download.\")\n",
    "print(\"All files are downloaded and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8749ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Load the datasets\n",
    "train_df = pd.read_csv('../raw_data/jigsaw-toxic-comment-classification-challenge/train/train.csv')\n",
    "cleaned_train_df = pd.read_csv('../processed_data/jigsaw_unintended/cleaned_train.csv')\n",
    "\n",
    "# # Inspect train.csv\n",
    "# print(\"=== train.csv Analysis ===\")\n",
    "# print(\"\\nShape:\", train_df.shape)\n",
    "# print(\"\\nColumns:\", train_df.columns.tolist())\n",
    "# print(\"\\nSample Data:\")\n",
    "# print(train_df.head())\n",
    "# print(\"\\nData Info:\")\n",
    "# print(train_df.info())\n",
    "# print(\"\\nMissing Values:\")\n",
    "# print(train_df.isnull().sum())\n",
    "\n",
    "# print(\"\\n\\n=== cleaned_train.csv Analysis ===\")\n",
    "# print(\"\\nShape:\", cleaned_train_df.shape)\n",
    "# print(\"\\nColumns:\", cleaned_train_df.columns.tolist())\n",
    "# print(\"\\nSample Data:\")\n",
    "# print(cleaned_train_df.head())\n",
    "# print(\"\\nData Info:\")\n",
    "# print(cleaned_train_df.info())\n",
    "# print(\"\\nMissing Values:\")\n",
    "# print(cleaned_train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88082964",
   "metadata": {},
   "source": [
    "Based on the analysis of both datasets, I can help suggest a strategy for merging them. Here are the key observations:\n",
    "\n",
    "Dataset Sizes:\n",
    "\n",
    "train.csv: 159,571 rows × 8 columns\n",
    "cleaned_train.csv: 1,804,874 rows × 45 columns\n",
    "Common Features:\n",
    "\n",
    "Both have 'id' and 'comment_text' columns\n",
    "Both have toxicity-related columns but with some differences:\n",
    "train.csv: toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "cleaned_train.csv: target, severe_toxicity, obscene, identity_attack, insult, threat\n",
    "Key Differences:\n",
    "\n",
    "cleaned_train.csv has additional demographic and metadata columns\n",
    "Column names are slightly different (e.g., 'severe_toxic' vs 'severe_toxicity')\n",
    "Data types differ ('id' is object in train.csv but int64 in cleaned_train.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "755b7b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Dataset Info:\n",
      "Shape: (1964445, 9)\n",
      "\n",
      "Sample of combined data:\n",
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...    0.0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...    0.0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...    0.0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...    0.0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...    0.0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate    source  \n",
      "0           0.0      0.0     0.0     0.0            0.0  original  \n",
      "1           0.0      0.0     0.0     0.0            0.0  original  \n",
      "2           0.0      0.0     0.0     0.0            0.0  original  \n",
      "3           0.0      0.0     0.0     0.0            0.0  original  \n",
      "4           0.0      0.0     0.0     0.0            0.0  original  \n",
      "\n",
      "Distribution of sources:\n",
      "source\n",
      "cleaned     1804874\n",
      "original     159571\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saving combined dataset to ../processed_data/combined_toxic_comments.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('../raw_data/jigsaw-toxic-comment-classification-challenge/train/train.csv')\n",
    "cleaned_train_df = pd.read_csv('../processed_data/jigsaw_unintended/cleaned_train.csv')\n",
    "\n",
    "# Since there's no ID overlap, we should:\n",
    "# 1. Create a combined dataset using concatenation instead of merge\n",
    "# 2. Add a source column to track the origin\n",
    "# 3. Standardize column names before combining\n",
    "\n",
    "# Add source column\n",
    "train_df['source'] = 'original'\n",
    "cleaned_train_df['source'] = 'cleaned'\n",
    "\n",
    "# Rename columns in cleaned_df to match train_df\n",
    "column_mapping = {\n",
    "    'severe_toxicity': 'severe_toxic',\n",
    "    'identity_attack': 'identity_hate',\n",
    "    'target': 'toxic'  # Assuming 'target' in cleaned_df corresponds to 'toxic' in train_df\n",
    "}\n",
    "\n",
    "cleaned_train_df = cleaned_train_df.rename(columns=column_mapping)\n",
    "\n",
    "# Select common columns for concatenation\n",
    "common_columns = ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', \n",
    "                 'threat', 'insult', 'identity_hate', 'source']\n",
    "\n",
    "# Ensure all common columns exist in both dataframes\n",
    "for col in common_columns:\n",
    "    if col not in train_df.columns:\n",
    "        train_df[col] = None\n",
    "    if col not in cleaned_train_df.columns:\n",
    "        cleaned_train_df[col] = None\n",
    "\n",
    "# Concatenate datasets\n",
    "combined_df = pd.concat([\n",
    "    train_df[common_columns], \n",
    "    cleaned_train_df[common_columns]\n",
    "], axis=0, ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nCombined Dataset Info:\")\n",
    "print(f\"Shape: {combined_df.shape}\")\n",
    "print(\"\\nSample of combined data:\")\n",
    "print(combined_df.head())\n",
    "print(\"\\nDistribution of sources:\")\n",
    "print(combined_df['source'].value_counts())\n",
    "\n",
    "# Save combined dataset only if it doesn't exist\n",
    "output_path = '../processed_data/combined_toxic_comments.csv'\n",
    "if not os.path.exists(output_path):\n",
    "    print(f\"\\nSaving combined dataset to {output_path}\")\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "else:\n",
    "    print(f\"\\nFile already exists at {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da80fcc",
   "metadata": {},
   "source": [
    "Dataset Combination Analysis\n",
    "Looking at the results, the combination was successful! Here's what we achieved:\n",
    "\n",
    "Combined Dataset Size:\n",
    "\n",
    "Total rows: 1,964,445 (matches sum of both datasets)\n",
    "Columns: 9 (common columns + source)\n",
    "Source Distribution:\n",
    "\n",
    "Original dataset: 159,571 rows\n",
    "Cleaned dataset: 1,804,874 rows\n",
    "Data Structure:\n",
    "\n",
    "All toxicity-related columns standardized\n",
    "Source column added for tracking\n",
    "No missing values in key columns\n",
    "The code worked as intended, creating a combined dataset that:\n",
    "\n",
    "Preserves all records from both sources\n",
    "Maintains consistent column names\n",
    "Tracks the origin of each record\n",
    "Handles the data type differences\n",
    "If you want to verify the data quality, here's a suggested quality check code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d16d993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality Check Results:\n",
      "\n",
      "1. Missing Values:\n",
      "id                 0\n",
      "comment_text     223\n",
      "toxic              0\n",
      "severe_toxic       0\n",
      "obscene            0\n",
      "threat             0\n",
      "insult             0\n",
      "identity_hate      0\n",
      "source             0\n",
      "dtype: int64\n",
      "\n",
      "2. Value Ranges:\n",
      "\n",
      "toxic:\n",
      "count    1.964445e+06\n",
      "mean     1.024346e-01\n",
      "std      2.067052e-01\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      1.666667e-01\n",
      "max      1.000000e+00\n",
      "Name: toxic, dtype: float64\n",
      "\n",
      "severe_toxic:\n",
      "count    1.964445e+06\n",
      "mean     5.021831e-03\n",
      "std      3.586350e-02\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      1.000000e+00\n",
      "Name: severe_toxic, dtype: float64\n",
      "\n",
      "obscene:\n",
      "count    1.964445e+06\n",
      "mean     1.705093e-02\n",
      "std      8.956470e-02\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      1.000000e+00\n",
      "Name: obscene, dtype: float64\n",
      "\n",
      "threat:\n",
      "count    1.964445e+06\n",
      "mean     8.798247e-03\n",
      "std      4.989708e-02\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      1.000000e+00\n",
      "Name: threat, dtype: float64\n",
      "\n",
      "insult:\n",
      "count    1.964445e+06\n",
      "mean     7.857052e-02\n",
      "std      1.799121e-01\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      1.000000e+00\n",
      "Name: insult, dtype: float64\n",
      "\n",
      "identity_hate:\n",
      "count    1.964445e+06\n",
      "mean     2.151224e-02\n",
      "std      8.011454e-02\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      1.000000e+00\n",
      "Name: identity_hate, dtype: float64\n",
      "\n",
      "3. Sample from each source:\n",
      "\n",
      "Original source sample:\n",
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...    0.0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...    0.0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate    source  \n",
      "0           0.0      0.0     0.0     0.0            0.0  original  \n",
      "1           0.0      0.0     0.0     0.0            0.0  original  \n",
      "\n",
      "Cleaned source sample:\n",
      "           id                                       comment_text  toxic  \\\n",
      "159571  59848  this is so cool its like would you want your m...    0.0   \n",
      "159572  59849  thank you this would make my life a lot less a...    0.0   \n",
      "\n",
      "        severe_toxic  obscene  threat  insult  identity_hate   source  \n",
      "159571           0.0      0.0     0.0     0.0            0.0  cleaned  \n",
      "159572           0.0      0.0     0.0     0.0            0.0  cleaned  \n"
     ]
    }
   ],
   "source": [
    "# Add a new cell with these checks\n",
    "print(\"Quality Check Results:\")\n",
    "print(\"\\n1. Missing Values:\")\n",
    "print(combined_df.isnull().sum())\n",
    "\n",
    "print(\"\\n2. Value Ranges:\")\n",
    "for col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(combined_df[col].describe())\n",
    "\n",
    "print(\"\\n3. Sample from each source:\")\n",
    "print(\"\\nOriginal source sample:\")\n",
    "print(combined_df[combined_df['source'] == 'original'].head(2))\n",
    "print(\"\\nCleaned source sample:\")\n",
    "print(combined_df[combined_df['source'] == 'cleaned'].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0bb5e",
   "metadata": {},
   "source": [
    "Data Quality Analysis\n",
    "Based on the quality check results, here's a detailed analysis of the combined dataset:\n",
    "\n",
    "1. Missing Values\n",
    "Only 223 missing values in comment_text column (0.01% of data)\n",
    "All other columns are complete\n",
    "Recommendation: Consider handling missing comments before modeling\n",
    "2. Class Distribution\n",
    "For toxic labels (mean values):\n",
    "\n",
    "toxic: 10.24% of comments\n",
    "severe_toxic: 0.50% of comments\n",
    "obscene: 1.71% of comments\n",
    "threat: 0.88% of comments\n",
    "insult: 7.86% of comments\n",
    "identity_hate: 2.15% of comments\n",
    "Note: Significant class imbalance across all categories\n",
    "\n",
    "3. Value Ranges\n",
    "All toxicity columns show:\n",
    "\n",
    "Min: 0.0\n",
    "Max: 1.0\n",
    "Majority of values at 0 (75th percentile is 0 for most categories)\n",
    "toxic has more spread (75th percentile at 0.167)\n",
    "4. Source Distribution\n",
    "Successfully tracked data origins:\n",
    "\n",
    "Original: 159,571 records\n",
    "Cleaned: 1,804,874 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6367ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use this for further processing or model training\n",
    "# # Handle missing values in comment_text\n",
    "# combined_df['comment_text'] = combined_df['comment_text'].fillna('[MISSING_COMMENT]')\n",
    "\n",
    "# # Add length features\n",
    "# combined_df['comment_length'] = combined_df['comment_text'].str.len()\n",
    "# combined_df['word_count'] = combined_df['comment_text'].str.split().str.len()\n",
    "\n",
    "# # Add toxicity summary features\n",
    "# combined_df['total_toxicity'] = combined_df[['toxic', 'severe_toxic', 'obscene', \n",
    "#                                            'threat', 'insult', 'identity_hate']].sum(axis=1)\n",
    "# combined_df['toxicity_types'] = combined_df[['toxic', 'severe_toxic', 'obscene', \n",
    "#                                            'threat', 'insult', 'identity_hate']].gt(0).sum(axis=1)\n",
    "\n",
    "# # Save enhanced dataset\n",
    "# if not os.path.exists(output_path):\n",
    "#     combined_df.to_csv(output_path, index=False)\n",
    "#     print(f\"Enhanced dataset saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
