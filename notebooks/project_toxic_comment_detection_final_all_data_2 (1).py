# -*- coding: utf-8 -*-
"""Project_toxic_Comment_detection_final_all_data-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QTCwz1V8UAldJaGJIzoH1FW-gLwZ-dfN
"""

import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('stopwords')

nltk.download('wordnet')

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to your file in Google Drive
file_path = '/content/drive/My Drive/NLP/train.csv'

# Load the file (assuming it's a CSV)
import pandas as pd
df_train = pd.read_csv(file_path)

# Display the first few rows of the dataset
print(df_train.head())

df_train.head()

df_train.describe()

df_train.isnull().sum()

all_labels=['toxic','severe_toxic','obscene','threat','insult','identity_hate']



# Sum each label across all rows to get the count for each label
label_counts = df_train[all_labels].sum()

plt.figure(figsize=(10,8))
plt.bar(x=all_labels, height=label_counts)
plt.xlabel('Labels')
plt.ylabel('Count')
plt.title('Label Count')
plt.xticks(rotation=45)
plt.show()

df_train[(df_train['toxic']!=1) & ((df_train['severe_toxic']==1)|(df_train['obscene']==1) | (df_train['threat']==1) | (df_train['insult']==1) | (df_train['identity_hate']==1) )]

# Here I can see that the the comments that are not clssified as toxic are classified as obscene, threadtm insult and identity hate

df_train[(df_train['toxic']!=1) & (df_train['severe_toxic']==1)]

# Also toxic comment are severe_toxic. So I'll be dropping this column

df_train[(df_train['toxic']==1) | (df_train['severe_toxic']==1)|(df_train['obscene']==1) | (df_train['threat']==1) | (df_train['insult']==1) | (df_train['identity_hate']==1) ]

df_train[(df_train['toxic']!=1) & (df_train['threat']==1)]

df_train['None']=0

# Corrected assignment
df_train.loc[(df_train['toxic'] == 0) &
             (df_train['severe_toxic'] == 0) &
             (df_train['obscene'] == 0) &
             (df_train['threat'] == 0) &
             (df_train['insult'] == 0) &
             (df_train['identity_hate'] == 0), 'None'] = 1

df_train[df_train['None']==1]

all_labels=['toxic','severe_toxic','obscene','threat','insult','identity_hate','None']

label_counts = df_train[all_labels].sum()
label_counts

# Sum each label across all rows to get the count for each label


plt.figure(figsize=(10,8))
plt.bar(x=all_labels, height=label_counts)
plt.xlabel('Labels')
plt.ylabel('Count')
plt.title('Label Count')
plt.xticks(rotation=45)
plt.show()

df_train['comment_text'].count()

text_data=df_train['comment_text']

text_data

comment_length=text_data.apply(len)

comment_length

plt.figure(figsize=(10,8))
plt.hist(x=comment_length,bins=500,color='lightblue',edgecolor='black')
plt.xlabel('comment_length')
plt.ylabel('frequency')
plt.show()

# Give an idea that the most comment length is between 0 to 700

# Now seeing heatmap to see how each labels relate to each other.

sns.heatmap(data=df_train[all_labels].corr(),cmap='coolwarm',annot=True)

# Setting new tooxicity for the columns to store the data for all type of toxicity
# Corrected assignment
df_train['all_kind_of_toxic_or_not']=0
df_train.loc[df_train['None']==0,'all_kind_of_toxic_or_not']=1

counts = df_train[['None', 'all_kind_of_toxic_or_not']].sum()
plt.figure(figsize=(10,8))
plt.bar(x=counts.index,height=counts)

df_train[df_train['all_kind_of_toxic_or_not']==1]

X=df_train['comment_text']
y=df_train['all_kind_of_toxic_or_not']

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
def preprocess(row):
    row = row.lower()
    row = re.sub(r'[^\w\s]', '', row)
    words = row.split()
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return ' '.join(words) if words else 'empty'

def get_features(X_train,train_data=False):
    X=[]
    for row in X_train:
        row=preprocess(row)
        X.append(row)
    if(train_data==True):
        X_tfidf=tfidf.fit_transform(X)
    else:
        X_tfidf=tfidf.transform(X)
    return X_tfidf

def train_model(X_train,y_train,model):
    X=get_features(X_train,train_data=True)
    model.fit(X,y_train)
    return model

X_train,X_test,y_train,y_pred=train_test_split(X,y,test_size=0.3,train_size=0.7)
tfidf = TfidfVectorizer(stop_words='english',ngram_range=(1, 2))
LR_model=LogisticRegression(random_state=42,max_iter=1000,class_weight='balanced',C=2)
model=train_model(X_train,y_train,LR_model)

def model_predict(trained_model,X_test,y_test):
    X=get_features(X_test,train_data=False)
    predictions=trained_model.predict(X)
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions)
    recall = recall_score(y_test, predictions)
    f1 = f1_score(y_test, predictions)
    cm=confusion_matrix(y_test,predictions)
    # Print metrics for analysis
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 Score: {f1:.2f}")
    print(cm)
    tn, fp, fn, tp = confusion_matrix(y_test,predictions).ravel()
    print(tn, fp, fn, tp)

model_predict(model,X_test,y_pred)

#X_train,X_test,y_train,y_pred=train_test_split(X,y,test_size=0.3,train_size=0.7)
#tfidf = TfidfVectorizer(stop_words='english')
nb=MultinomialNB()
model=train_model(X_train,y_train,nb)

model_predict(model,X_test,y_pred)

x_preprocessed = [preprocess(row) for row in X]

train_x, eval_x, train_y, eval_y=train_test_split(x_preprocessed,y,test_size=0.3,train_size=0.7)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from transformers import AutoTokenizer, BertModel
import torch

train_data = {'text': train_x, 'labels': train_y}
train_dataset = Dataset.from_dict(train_data)

eval_data = {'text': eval_x, 'labels': eval_y}
eval_dataset = Dataset.from_dict(eval_data)

# Define the model name
model_name = "bert-base-uncased"

# Load the tokenizer and model using Auto classes
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenize the train and eval datasets
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=True)

# Apply tokenization
train_dataset = train_dataset.map(tokenize_function, batched=True)
eval_dataset = eval_dataset.map(tokenize_function, batched=True)

import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
model.to(device)

from transformers import EarlyStoppingCallback

# Define the compute_metrics function
def compute_metrics(p):
    preds, labels = p
    preds = preds.argmax(axis=-1)  # Convert logits to predicted class labels
    accuracy = accuracy_score(labels, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }
# Define training arguments with logging and evaluation strategy
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",  # set to 'steps'
    logging_dir="./logs",  # Directory to store logs
    logging_steps=500,  # Log every 100 steps
    save_strategy="epoch",  # Save checkpoint every 'steps'
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=6,
    weight_decay=0.01, # This will allow me to visualize the metrics with TensorBoard
    load_best_model_at_end=True, # Load best model after training
    metric_for_best_model="eval_loss",  # Metric to monitor
    greater_is_better=False,      # Lower eval_loss is better
    report_to="tensorboard"  # Report metrics to TensorBoard
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

# Train the model
trainer.train()

# Get predictions and true labels from the eval dataset
predictions = trainer.predict(eval_dataset)
predicted_labels = predictions.predictions.argmax(axis=-1)
true_labels = predictions.label_ids

# Calculate metrics
accuracy = accuracy_score(true_labels, predicted_labels)
precision = precision_score(true_labels, predicted_labels)
recall = recall_score(true_labels, predicted_labels)
f1 = f1_score(true_labels, predicted_labels)
conf_matrix = confusion_matrix(true_labels, predicted_labels)

# Print the results
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Confusion Matrix:\n{conf_matrix}")
tn, fp, fn, tp = conf_matrix.ravel()
print(tn, fp, fn, tp)



