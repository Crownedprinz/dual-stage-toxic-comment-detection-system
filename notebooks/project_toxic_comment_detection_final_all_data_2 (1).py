# -*- coding: utf-8 -*-
"""Project_toxic_Comment_detection_final_all_data_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JA7ejtfZGH9vfUGE8cGp2YaOv9RsNgWN
"""

import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('stopwords')

nltk.download('wordnet')

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to your file in Google Drive
file_path = '/content/drive/My Drive/NLP/train.csv'

# Load the file (assuming it's a CSV)
import pandas as pd
df_train = pd.read_csv(file_path)

# Display the first few rows of the dataset
print(df_train.head())

df_train.head()

df_train.describe()

df_train.isnull().sum()

all_labels=['toxic','severe_toxic','obscene','threat','insult','identity_hate']

# Sum each label across all rows to get the count for each label
label_counts = df_train[all_labels].sum()

plt.figure(figsize=(10,8))
plt.bar(x=all_labels, height=label_counts)
plt.xlabel('Labels')
plt.ylabel('Count')
plt.title('Label Count')
plt.xticks(rotation=45)
plt.show()

df_train[(df_train['toxic']!=1) & ((df_train['severe_toxic']==1)|(df_train['obscene']==1) | (df_train['threat']==1) | (df_train['insult']==1) | (df_train['identity_hate']==1) )]

# Here I can see that the the comments that are not clssified as toxic are classified as obscene, threadtm insult and identity hate

df_train[(df_train['toxic']!=1) & (df_train['severe_toxic']==1)]

# Also toxic comment are severe_toxic. So I'll be dropping this column

df_train[(df_train['toxic']==1) | (df_train['severe_toxic']==1)|(df_train['obscene']==1) | (df_train['threat']==1) | (df_train['insult']==1) | (df_train['identity_hate']==1) ]

df_train[(df_train['toxic']!=1) & (df_train['threat']==1)]

df_train['Non-toxic']=0

# Corrected assignment
df_train.loc[(df_train['toxic'] == 0) &
             (df_train['severe_toxic'] == 0) &
             (df_train['obscene'] == 0) &
             (df_train['threat'] == 0) &
             (df_train['insult'] == 0) &
             (df_train['identity_hate'] == 0), 'Non-toxic'] = 1

df_train[df_train['Non-toxic']==1]

all_labels=['toxic','severe_toxic','obscene','threat','insult','identity_hate','Non-toxic']

label_counts = df_train[all_labels].sum()
label_counts

# Sum each label across all rows to get the count for each label


plt.figure(figsize=(10,8))
plt.bar(x=all_labels, height=label_counts)
plt.xlabel('Labels')
plt.ylabel('Count')
plt.title('Label Count')
plt.xticks(rotation=45)
plt.show()

df_train['comment_text'].count()

text_data=df_train['comment_text']

text_data

comment_length=text_data.apply(len)

comment_length

plt.figure(figsize=(10,8))
plt.hist(x=comment_length,bins=500,color='lightblue',edgecolor='black')
plt.xlabel('comment_length')
plt.ylabel('frequency')
plt.show()

# Setting new tooxicity for the columns to store the data for all type of toxicity
# Corrected assignment
df_train['all_kind_of_toxic']=0
df_train.loc[df_train['Non-toxic']==0,'all_kind_of_toxic']=1

counts = df_train[['Non-toxic', 'all_kind_of_toxic']].sum()
plt.figure(figsize=(10,8))
plt.bar(x=counts.index,height=counts)

df_train[df_train['all_kind_of_toxic']==1]

X=df_train['comment_text']
y=df_train['all_kind_of_toxic']

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
def preprocess(row):
    row = row.lower()
    row = re.sub(r'[^\w\s]', '', row)
    words = row.split()
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return ' '.join(words) if words else 'empty'

def get_features(X_train,train_data=False):
    X=[]
    for row in X_train:
        row=preprocess(row)
        X.append(row)
    if(train_data==True):
        X_tfidf=tfidf.fit_transform(X)
    else:
        X_tfidf=tfidf.transform(X)
    return X_tfidf

def train_model(X_train,y_train,model):
    X=get_features(X_train,train_data=True)
    model.fit(X,y_train)
    return model

X_train,X_test,y_train,y_pred=train_test_split(X,y,test_size=0.3,train_size=0.7)
tfidf = TfidfVectorizer(stop_words='english',ngram_range=(1, 2))
LR_model=LogisticRegression(random_state=42,max_iter=1000,class_weight='balanced',C=2)
model=train_model(X_train,y_train,LR_model)

def model_predict(trained_model,X_test,y_test):
    X=get_features(X_test,train_data=False)
    predictions=trained_model.predict(X)
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions)
    recall = recall_score(y_test, predictions)
    f1 = f1_score(y_test, predictions)
    cm=confusion_matrix(y_test,predictions)
    # Print metrics for analysis
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 Score: {f1:.2f}")
    print(cm)
    tn, fp, fn, tp = confusion_matrix(y_test,predictions).ravel()
    print(tn, fp, fn, tp)
    # Plot Confusion Matrix
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="RdPu", xticklabels=["all_kind_of_toxic", "Non-toxic"], yticklabels=["all_kind_of_toxic", "Non-toxic"])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix")
    plt.show()

model_predict(model,X_test,y_pred)

#X_train,X_test,y_train,y_pred=train_test_split(X,y,test_size=0.3,train_size=0.7)
#tfidf = TfidfVectorizer(stop_words='english')
nb=MultinomialNB()
model=train_model(X_train,y_train,nb)

model_predict(model,X_test,y_pred)
